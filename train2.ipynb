{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from utils import Tokenizer \n",
    "import utils\n",
    "from config import chars\n",
    "import random\n",
    "from model2 import Transformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load data_dict from the JSON file\n",
    "with open(\"shart_dict.json\", \"r\") as f:\n",
    "    data_dict = json.load(f)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load training parameters\n",
    "batch_size, learning_rate, epochs, char_size, d_model, n_heads, dropout_rate, head_size, n_layers = utils.get_train_params() #take out head_size\n",
    "n_fft, hop_length, sr, n_mels, max_timesteps = utils.get_audio_params()\n",
    "training_mels, training_text = utils.get_training_data()\n",
    "\n",
    "# Define custom dataset class\n",
    "class TTSDataSet(Dataset):\n",
    "    def __init__(self, keys, training_mels, training_text, max_timesteps):\n",
    "        self.keys = keys\n",
    "        self.mels_data_path = training_mels\n",
    "        self.text_data_path = training_text\n",
    "        self.max_timesteps = max_timesteps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "        key = self.keys[idx]\n",
    "        txt = torch.load(f\"{self.text_data_path}/{key}.pt\", weights_only=True) #text S\n",
    "        tgt = torch.load(f\"{self.mels_data_path}/{key}.pt\", weights_only=True) #audio S,C\n",
    "        txt = torch.nn.functional.pad(txt, (0, tgt.size(0) - txt.size(0)), value=4)  #this adds special filler tokens \"FILL\" to align txt text (S,) dimension with tgt audio dimension (S,)\n",
    "        txt = torch.nn.functional.pad(txt, (0, self.max_timesteps - txt.size(0)), value=0) #adds padding to max timesteps \n",
    "        tgt = torch.nn.functional.pad(tgt, (0, 0, 0, self.max_timesteps - tgt.size(0)), value=-3) # s,c where padded s gets -3 and c relating to the rows get -3\n",
    "        return txt, tgt\n",
    "\n",
    "# Define collate function\n",
    "def collate_fn(batch):\n",
    "    txt_batch, tgt_batch = zip(*batch)  \n",
    "    txt_padded = torch.stack(txt_batch) # txt -> (B, S), \n",
    "    tgt_padded = torch.stack(tgt_batch) # tgt -> (B, S, C)\n",
    "    attn_mask = (tgt_padded != -3).any(dim=-1)  # Shape: (B, max_timesteps) - attn mask has true for non-padded positions\n",
    "    valid_lengths = attn_mask.sum(dim=-1)  # Shape: (B,), number of valid timesteps per sequence\n",
    "    # Apply 70-100% contiguous masking only to the valid portion\n",
    "    random_mask = torch.zeros_like(attn_mask, dtype=torch.bool)  # Initialize with all False\n",
    "    for i, valid_len in enumerate(valid_lengths):\n",
    "        mask_percentage = random.uniform(0.7, 1.0)  # Random percentage between 70% and 100%\n",
    "        timesteps_to_mask = int(mask_percentage * valid_len)  # Compute timesteps to mask\n",
    "        temp_mask = torch.zeros(valid_len, dtype=torch.bool)             # Generate a contiguous mask for valid timesteps\n",
    "        start_idx = valid_len - timesteps_to_mask\n",
    "        temp_mask[start_idx:] = True  # Mask the first `timesteps_to_mask` elements            \n",
    "        random_mask[i, :valid_len] = temp_mask  # Apply the contiguous mask to the valid region\n",
    "    audio_mask = attn_mask & ~random_mask  # Keep valid positions, excluding masked ones (B,S), the random mask has true for the masked positions and gets inverted here to false\n",
    "    return txt_padded, tgt_padded, audio_mask, attn_mask\n",
    "\n",
    "#Prepare data and dataloader\n",
    "data_keys = list(data_dict.keys())\n",
    "random.shuffle(data_keys)\n",
    "dataset = TTSDataSet(data_keys, training_mels, training_text, max_timesteps)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: collate_fn(batch), pin_memory=True)\n",
    "\n",
    "\n",
    "output_dir = \"/home/kunit17/Her/Data/TrainingOutput\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11855', '2122', '6822', '8804', '3490', '10670', '10246', '11730', '4797', '1974', '2793', '9845', '2604', '7170', '7912', '12512', '3169', '5104', '9941', '2373', '2386', '11876', '9224', '2653', '2598']\n",
      "328177764 M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from utils import Tokenizer \n",
    "import utils\n",
    "from config import chars\n",
    "import random\n",
    "from model2 import Transformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "print(data_keys[:25])\n",
    "char_size, d_model, n_heads, dropout_rate, n_layers, n_mels, device = 64,1024,16,.001,24,100,'cuda'\n",
    "# Initialize model, tokenizer, and optimizer\n",
    "model = Transformer(char_size, d_model, n_heads, n_layers, n_mels, dropout_rate, device) \n",
    "model = model.to(device)\n",
    "# model = torch.compile(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "print(sum(p.numel() for p in model.parameters()), 'M parameters')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 entries of tgt_padded:\n",
      "\n",
      "First 5 entries of tgt_mask:\n",
      "torch.Size([32, 556]) tensor([[False, False, False, False, False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# Get a single batch from the dataloader\n",
    "for batch in dataloader:\n",
    "    txt_padded, tgt_padded, audio_mask, attn_mask = batch\n",
    "\n",
    "    # Print the first 5 entries of tgt_padded and tgt_mask\n",
    "    print(\"First 5 entries of tgt_padded:\")\n",
    "    #print(tgt_padded.shape,tgt_padded[1][:1])  # Display first 5 entries\n",
    "\n",
    "    print(\"\\nFirst 5 entries of tgt_mask:\")\n",
    "    print(audio_mask.shape, attn_mask[0:1,500:510])  # Display first 5 entries\n",
    "\n",
    "    #Break after printing one batch txt_padded, tgt_padded, audio_mask, attn_mask\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "import time\n",
    "\n",
    "mini_batch_size = 294912 # close to E2TTS 302,700\n",
    "num_micro_epochs = mini_batch_size // (batch_size * max_timesteps)\n",
    "output_dir = f'Her/model_checkpoint'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    t0 = time.time()\n",
    "    last_step = (epoch==epochs-1)    \n",
    "    epoch_loss = 0.0\n",
    "    saved = False  # Ensure only one mel_output is saved per epoch\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0\n",
    "    mini_batch_tracker = 0 \n",
    "    for txt, audio_targets, audio_mask, attn_mask in dataloader:\n",
    "\n",
    "        text = text.to(device, non_blocking=True)\n",
    "        x1 = audio_targets.to(device, non_blocking=True)  #target distribution\n",
    "        audio_mask = audio_mask.to(device, non_blocking=True)\n",
    "        attn_mask = attn_mask.to(device, non_blocking=True)\n",
    "        t = torch.rand(batch_size, device=device).view(batch_size,1,1) #(B,1,1)\n",
    "\n",
    "        grad_accum_steps = mini_batch_size // (batch_size * max_timesteps)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            vt, flow = model(x1, t, text, audio_mask=audio_mask, attn_mask=attn_mask)\n",
    "            loss = nn.functional.mse_loss(vt,flow, reduction='none') #returns element-wise squared diff for each pair\n",
    "            loss = loss * attn_mask.unsqueeze(-1)\n",
    "            final_loss = loss.sum() / (attn_mask.sum() * vt.shape[-1]) # need to factor in total elements - check that loss.sum matches attn_mask elements Number of\n",
    "            #loss = loss / grad_accum_steps\n",
    "\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "        mini_batch_tracker += 1\n",
    "        if mini_batch_tracker == num_micro_epochs + 1:\n",
    "            break\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n",
    "    lr = 0.0001 #get_lr(step) #determine and set learning rate for this iteration 0.0001\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "\n",
    "# Save the model weights at the end of training\n",
    "if last_step:\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model_save_path = os.path.join(output_dir, \"final_model_weights.pth\")\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model weights saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference Loops\n",
    "#put model into train mode\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NFE = 32\n",
    "time_steps = torch.linspace(0,1, NFE // 2).to(device)\n",
    "txt = \"I miss you already, baby\"\n",
    "model.eval()\n",
    "#tokenize(txt)\n",
    "xt = 1 #insert sample mel-spec\n",
    "for i in range(time_steps):\n",
    "    xt = model.inference_step(txt=txt, xt=xt, t_start = time_steps[i], t_end = time_steps[i+1])\n",
    "\n",
    "#don't forget to permute\n",
    "# xt is normalizd, log mel spec - need to unnormalize\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
