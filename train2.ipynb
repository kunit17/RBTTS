{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from utils import Tokenizer \n",
    "import utils\n",
    "from config import chars\n",
    "import random\n",
    "from model2 import Transformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load data_dict from the JSON file\n",
    "with open(\"data_dict.json\", \"r\") as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "# Set manual seed and device\n",
    "torch.manual_seed(1337)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load training parameters\n",
    "batch_size, learning_rate, epochs, block_size, char_size, d_model, n_heads, dropout_rate, head_size = utils.get_train_params() #take out head_size\n",
    "n_layers = 5\n",
    "n_mels = 128\n",
    "max_timesteps = 302\n",
    "data_path = utils.get_training_data()\n",
    "velocity_consistency_delta = 1e-5\n",
    "\n",
    "# Define custom dataset class\n",
    "class TTSDataSet(Dataset):\n",
    "    def __init__(self, keys, data_path, block_size, max_timesteps):\n",
    "        self.keys = keys\n",
    "        self.data_path = data_path\n",
    "        self.max_timesteps = max_timesteps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "        key = self.keys[idx]\n",
    "        src = torch.load(f\"{self.data_path}/{key}_src_idx.pt\", weights_only=True) #text S\n",
    "        tgt = torch.load(f\"{self.data_path}/{key}_mel.pt\", weights_only=True) #audio S,C\n",
    "        src = torch.nn.functional.pad(src, (0, tgt.size(0) - src.size(0)), value=4)  #this adds special filler tokens \"FILL\" to align src text (S,) dimension with tgt audio dimension (S,)\n",
    "        src = torch.nn.functional.pad(src, (0, self.max_timesteps - src.size(0)), value=0) #adds padding to max timesteps\n",
    "        tgt = torch.nn.functional.pad(tgt, (0, 0, 0, self.max_timesteps - tgt.size(0)), value=-100)\n",
    "        return src, tgt\n",
    "\n",
    "# Define collate function\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)  # src -> (B, S), tgt -> (B, S, C)\n",
    "    src_padded = torch.stack(src_batch)\n",
    "    tgt_padded = torch.stack(tgt_batch) \n",
    "    src_mask = (src_padded != 0)  # True where not padded\n",
    "\n",
    "    # Base mask: Identify valid (non-padded) positions\n",
    "    valid_mask = (tgt_padded != -100).any(dim=-1)  # Shape: (B, max_timesteps)\n",
    "\n",
    "    # Calculate valid lengths\n",
    "    valid_lengths = valid_mask.sum(dim=-1)  # Shape: (B,), number of valid timesteps per sequence\n",
    "\n",
    "    # Apply 70-100% contiguous masking only to the valid portion\n",
    "    random_mask = torch.zeros_like(valid_mask, dtype=torch.bool)  # Initialize with all False\n",
    "    for i, valid_len in enumerate(valid_lengths):\n",
    "        if valid_len > 0:  # Ensure there are valid timesteps\n",
    "            mask_percentage = random.uniform(0.7, 1.0)  # Random percentage between 70% and 100%\n",
    "            timesteps_to_mask = int(mask_percentage * valid_len)  # Compute timesteps to mask\n",
    "\n",
    "            # Generate a contiguous mask for valid timesteps\n",
    "            temp_mask = torch.zeros(valid_len, dtype=torch.bool, device=tgt_padded.device)\n",
    "            temp_mask[:timesteps_to_mask] = True  # Mask the first `timesteps_to_mask` elements\n",
    "\n",
    "            # Apply the contiguous mask to the valid region\n",
    "            random_mask[i, :valid_len] = temp_mask \n",
    "\n",
    "    # Combine the original valid mask with the random mask\n",
    "    audio_mask = valid_mask & ~random_mask  # Keep valid positions, excluding masked ones (B,S)\n",
    "\n",
    "    return src_padded, tgt_padded, src_mask, audio_mask\n",
    "\n",
    "# Prepare data and dataloader\n",
    "data_keys = list(data_dict.keys())\n",
    "random.shuffle(data_keys)\n",
    "dataset = TTSDataSet(data_keys, data_path, block_size, max_timesteps)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: collate_fn(batch), pin_memory=True)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "output_dir = \"/home/kunit17/Her/Data/TrainingOutput\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model, tokenizer, and optimizer\n",
    "model = Transformer(block_size, char_size, d_model, n_heads, dropout_rate, n_layers, n_mels) \n",
    "model = model.to(device)\n",
    "model = torch.compile(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "print(sum(p.numel() for p in model.parameters()), 'M parameters')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch from the dataloader\n",
    "for batch in dataloader:\n",
    "    src_padded, tgt_padded, src_mask, audio_mask = batch\n",
    "\n",
    "    # Print the first 5 entries of tgt_padded and tgt_mask\n",
    "    print(\"First 5 entries of tgt_padded:\")\n",
    "    #print(tgt_padded.shape,tgt_padded[1][:3])  # Display first 5 entries\n",
    "\n",
    "    print(\"\\nFirst 5 entries of tgt_mask:\")\n",
    "    print(tgt_mask.shape, tgt_mask[:2])  # Display first 5 entries\n",
    "\n",
    "    # Break after printing one batch\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m flow \u001b[38;5;241m=\u001b[39m x1 \u001b[38;5;241m-\u001b[39m x0\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m mel_output, loss \u001b[38;5;241m=\u001b[39m model(batch_src_idx, encoder_mask, decoder_input, decoder_mask, batch_targets)\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):    \n",
    "    epoch_loss = 0.0\n",
    "    saved = False  # Ensure only one mel_output is saved per epoch\n",
    "    for txt, audio_targets, txt_mask, audio_mask in dataloader:\n",
    "        \n",
    "        text = text.to(device, non_blocking=True)\n",
    "        x1 = audio_targets.to(device, non_blocking=True)  #target distribution\n",
    "        audio_mask = audio_mask.to(device, non_blocking=True)\n",
    "        x0 = torch.randn_like(x1) #Gaussian noise\n",
    "        times = torch.rand(batch_size, device=device).view(batch_size,1,1) #(B,1,1)\n",
    "        t = times * (1. - velocity_consistency_delta)\n",
    "        xt = (1. - t) * x0 + t * x1 #interpolated training sample/noisy speech\n",
    "        flow = x1 - x0\n",
    "        cond = torch.where(audio_mask.unsqueeze(-1), x1, torch.zeros_like(x1)) #masked speech\n",
    "        pred = model(xt, cond, times, text, audio_mask, txt_mask)\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        mel_output, loss = model()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if not saved:\n",
    "            random_idx = random.randint(0, mel_output.size(0) - 1)\n",
    "            random_mel_output = mel_output[random_idx].detach().cpu()\n",
    "            save_path = os.path.join(output_dir, f\"mel_output_epoch_{epoch}.pth\")\n",
    "            torch.save(random_mel_output, save_path)\n",
    "            print(f\"Saved mel_output for epoch {epoch} to {save_path}\")\n",
    "            saved = True  # Ensure only one mel_output is saved per epoch\n",
    "    print(f\"step {epoch:5d} | loss: {epoch_loss:.6f} | lr | norm:  | dt: {dt:.2f}s | tok/sec: \")\n",
    "        \n",
    "\n",
    "# Save the model weights at the end of training\n",
    "model_save_path = os.path.join(output_dir, \"final_model_weights.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model weights saved to {model_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
