{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Key with max 'Text' length: 359\n",
      "Text: 23 [\"i'll\", 'go', 'on', 'that', 'date', ',', 'fuck', 'her', 'brains', 'out', ',', 'show', 'you', 'how', \"it's\", 'done', '.', 'you', 'can', 'watch', 'and', 'cry', '.']\n",
      "['!', ',', '.', '000', '10', '100', '12', '13', '180', '1970s', '29th', '35', '360', '4', '40s', '5', '829', '86', '?', 'a', 'ab', 'ability', 'able', 'about', 'absurd', 'accept', 'acceptable', 'acoustic', 'across', 'act', 'actors', 'actually', 'address', 'adorable', 'adventure', 'advice', 'affectation', 'after', 'again', 'against', 'age', 'aggravated', 'ago', 'ah', 'alan', 'alien', 'alive', 'all', 'allow', 'allowing', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'altered', 'always', 'am', 'amazing', 'amy', 'an', 'anal', 'and', 'anger', 'angry', 'animal', 'annoyed', 'answer', 'anxious', 'any', 'anybody', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'apologize', 'are', 'area', 'argument', 'arguments', 'armpit', 'arms', 'around', 'article', 'artificial', 'artificially', 'as', 'asian', 'ask', 'asked', 'asleep', 'asshole', 'at', 'attack', 'attorney', 'available', 'aw', 'awake', 'away', 'awful', 'awkward', 'baby', 'back', 'background', 'bad', 'bare', 'bartender', 'based', 'basically', 'be', 'beach', 'beat', 'beautiful', 'became', 'because', 'become', 'becoming', 'bed', 'bedroom', 'been', 'beeping', 'beeps', 'before', 'behavioral', 'behind', 'being', 'believe', 'best', 'beta', 'better', 'between', 'big', 'billion', 'birthday', 'bit', 'bitch', 'black', 'blanket', 'bless', 'bloody', 'body', 'bonded', 'bonding', 'book', 'books', 'boring', 'both', 'bother', 'bothered', 'bouncy', 'bound', 'box', 'boyfriend', 'brain', 'brains', 'brainy', 'breakup', 'breasts', 'briefly', 'brilliant', 'bring', 'brings', 'brought', 'buddy', 'bullshit', 'business', 'busy', 'but', 'butthole', 'buzzes', 'by', 'bye', 'cables', 'california', 'call', 'called', 'calm', 'came', 'camera', 'can', \"can't\", 'canyon', 'captured', 'captures', 'card', 'care', 'carpool', 'carry', 'case', 'catalina', 'catherine', \"catherine's\", 'caught', 'caused', 'cells', 'challenges', 'challenging', 'change', 'changed', 'changing', \"chantels'\", 'chapter', 'charles', 'check', 'checking', 'cheek', 'cheers', 'cheese', 'chest', 'chick', 'choice', 'chuckle', 'chuckles', 'circles', 'class', 'clear', 'clearly', 'close', 'club', 'coffee', 'coke', 'college', 'columns', 'com', 'come', 'comes', 'comfortable', 'coming', 'commit', 'communicate', 'company', 'compiling', 'complete', 'complex', 'complicated', 'compliment', 'compose', 'computer', 'confuse', 'confused', 'confusing', 'contacts', 'context', 'continues', 'control', 'conversation', 'conversations', 'cool', 'corner', 'corrections', 'could', \"couldn't\", 'couple', \"couple's\", 'course', 'cracking', 'cracks', 'crazy', 'created', 'credit', 'creepy', 'critic', 'crooked', 'crown', 'cruise', 'cry', 'crying', 'cuddled', 'cuddling', 'cuddly', 'cum', 'cup', 'cupcakes', 'curious', 'cut', 'cute', 'da', 'dance', 'dark', 'darwinian', 'data', 'date', 'dated', 'dates', 'dating', 'day', 'dealing', 'dear', 'decision', 'deeply', 'defending', 'definitely', 'degrees', 'delete', 'delusional', 'dense', 'deny', 'describe', 'described', 'deserve', 'desk', 'destroy', 'dialogue', 'dick', 'did', \"didn't\", 'die', 'died', 'different', 'difficult', 'direction', 'disappeared', 'disappoint', 'discover', 'disorganized', 'distance', 'distract', 'distracted', 'divide', 'divorce', 'divorced', 'dna', 'do', 'documentaries', 'documentary', 'does', \"doesn't\", 'dog', 'doing', \"don't\", 'done', 'door', 'double', 'doubt', 'down', 'dozen', 'dragon', 'drank', 'drawing', 'dream', 'dreams', 'dress', 'dressed', 'drink', 'drive', 'drunk', 'dude', 'dumb', 'e', 'each', 'ear', 'earbud', 'earlier', 'earpiece', 'easier', 'easily', 'easy', 'eat', 'eight', 'ellie', 'else', \"else's\", 'embarrassing', 'emotions', 'end', 'ended', 'endless', 'ends', 'energy', 'enjoyed', 'enough', 'erase', 'even', 'evening', 'ever', 'every', 'everyone', 'everything', \"everything's\", 'everywhere', 'evolved', 'evolving', 'ex', 'exact', 'exactly', 'example', 'excited', 'exciting', 'excuse', 'existed', 'expands', 'experience', 'experiences', 'explain', 'explanation', 'explore', 'extra', 'eyes', 'face', 'fact', 'fail', 'fair', 'faking', 'fallen', 'falling', 'falls', 'familiar', 'fantasized', 'fantasizing', 'fantastic', 'far', 'faster', 'fat', 'father', 'fatty', 'favorite', 'favorites', 'fear', 'feel', 'feeling', 'feelings', 'feels', 'feet', 'fell', 'felt', 'few', 'fibers', 'figure', 'figured', 'filled', 'film', 'final', 'finally', 'find', 'finding', 'fine', 'fingers', 'finished', 'first', 'five', 'flow', 'follow', 'footage', 'for', 'forget', 'forgot', 'forgotten', 'form', 'formal', 'forty', 'forward', 'fought', 'found', 'four', 'freak', 'freaking', 'free', 'friend', 'friends', 'from', 'front', 'fruit', 'fruits', 'frustrating', 'fuck', 'fucker', 'fucking', 'fun', 'funny', 'fusion', 'fuzzy', 'game', 'games', 'gangly', 'gave', 'genius', 'get', 'gets', 'getting', 'girl', 'girlfriend', 'give', 'glad', 'go', 'god', \"goddaughter's\", 'going', 'gone', 'gonna', 'good', 'gorgeous', 'got', 'gotta', 'gotten', 'graduated', 'grammar', 'grandma', 'grateful', 'gray', 'great', 'grew', 'group', 'grow', 'growing', 'growling', 'guess', 'guilt', 'guitar', 'guy', 'guys', 'ha', 'had', 'half', 'halfway', 'handle', 'hands', 'happened', 'happening', 'happy', 'hard', 'harvard', 'has', 'hate', 'have', \"haven't\", 'having', 'he', \"he'd\", \"he's\", 'head', 'hear', 'heard', 'heart', \"heart's\", 'heartbreak', 'heavy', 'heh', 'hello', 'help', 'helped', 'helping', 'her', 'here', \"here's\", 'hers', 'hey', 'hi', 'hid', 'hilarious', 'him', 'hint', 'hired', 'his', 'hit', 'hitting', 'hm', 'hmm', 'hold', 'holding', 'hole', 'holy', 'home', 'honey', 'honeymoon', 'hope', 'horny', 'hot', 'hour', 'house', 'how', \"how'd\", \"how's\", 'hugged', 'huh', 'human', 'humans', 'hundred', 'hundredths', 'hungry', 'hurt', 'hurts', 'hyper', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'idea', 'ideas', 'if', 'imagine', 'imagined', 'important', 'in', 'inaudible', 'incredible', 'incredibly', 'inevitably', 'inferior', 'infinite', 'influence', 'inner', 'input', 'insane', 'insanity', 'inside', 'intellectual', 'intelligent', 'interesting', 'internet', 'interviewed', 'into', 'intuition', 'involved', 'is', 'isabella', 'ish', 'island', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'itch', 'its', 'jealous', 'jerk', 'job', 'jocelyn', \"john's\", 'joined', 'joke', 'joy', 'judgment', 'juice', 'juicing', 'just', 'keep', 'kettle', 'kevin', 'kid', 'kids', 'kill', 'kind', 'kiss', 'kissing', 'klausen', 'knew', 'knocking', 'know', 'knuckles', 'kook', 'l', 'la', 'lady', 'lampoon', 'lane', 'laptop', 'larger', 'last', 'late', 'lately', 'later', 'laude', 'laugh', 'laughing', 'laughs', 'lawyer', 'laying', 'learn', 'least', 'leave', 'leaving', 'left', 'less', 'lesser', 'let', \"let's\", 'letter', 'letters', 'lewman', 'liberating', 'liberty', 'lie', 'life', 'light', 'lights', 'like', 'liked', 'likes', 'limited', 'lip', 'lips', 'list', 'listen', 'listening', 'little', 'live', 'lives', 'living', 'lonely', 'long', 'longest', 'look', 'looking', 'looks', 'lose', 'lost', 'lot', 'loud', 'love', 'loved', 'lovely', 'loves', 'luck', 'luis', 'lunch', 'lying', 'made', 'madly', 'magna', 'mail', 'mailing', 'mails', 'major', 'make', 'makes', 'making', 'male', 'man', 'many', 'maria', 'mark', 'marked', 'marriage', 'married', 'marvel', 'mash', \"master's\", 'matter', 'maybe', 'me', 'mean', 'means', 'meant', 'meet', 'meeting', 'men', 'mess', 'messed', 'met', 'methods', 'michael', 'might', 'miles', 'million', 'millions', 'mimics', 'mind', 'mine', 'minute', 'minutes', 'misery', 'miss', 'missed', 'mission', 'mister', 'mixology', 'mm', 'moaning', 'moans', 'mockup', 'mom', 'moment', 'moms', 'money', 'months', 'moon', 'mopey', 'more', 'morning', 'most', 'mostly', 'mother', 'mountain', 'mouth', 'move', 'moved', 'mr', 'much', 'music', 'must', 'my', 'myself', 'name', 'names', 'neck', 'need', 'needed', 'never', 'new', 'next', 'nice', 'night', 'ninety', 'no', 'none', 'noodle', 'nope', 'normal', 'northern', 'nosy', 'not', 'nothing', 'now', 'obsessed', 'obviously', 'odd', 'of', 'off', 'office', 'oh', 'okay', 'old', 'on', 'one', 'ones', 'only', 'onto', 'operating', 'optimistic', 'or', 'orange', 'organism', 'organizational', 'os', 'oses', 'other', 'others', 'otherwise', 'our', 'ourselves', 'out', 'over', 'overwhelming', 'own', 'oxygen', 'pain', 'painful', 'panting', 'paper', 'papers', 'parents', 'part', 'partner', 'parts', 'past', 'paul', 'people', \"people's\", 'perceive', 'perceptive', 'perfect', 'person', 'personal', 'personalities', 'perspective', 'perv', 'petty', 'phase', 'phd', 'philosopher', 'phone', 'photo', 'photograph', 'photographs', 'phrases', 'physical', 'physics', 'piano', 'pick', 'picked', 'piece', 'pink', 'pit', 'place', 'planet', 'platform', 'play', 'playing', 'please', 'pleasure', 'poet', 'point', 'points', 'popular', 'porn', 'post', 'posting', 'prague', 'press', 'pretend', 'pretending', 'pretty', 'pricks', 'print', 'prioritize', 'probably', 'problem', 'problems', 'processed', 'processing', 'programmed', 'programmers', 'programming', 'project', 'proofread', 'prostitute', 'proud', 'provides', 'prozac', 'publish', 'publisher', 'pull', 'pulpy', 'punch', 'puppies', 'puppy', 'pure', 'pursued', 'push', 'pussy', 'put', 'putting', 'quickly', 'quiet', 'quivered', 'rachel', 'rack', 'random', 'rare', 'read', 'reading', 'ready', 'real', 'realize', 'realized', 'really', 'reason', 'rebuffs', 'recently', 'red', 'regarding', 'rehashing', 'relationship', 'relationships', 'relax', 'relaxed', 'relieved', 'remember', 'remembering', 'reminding', 'remotely', 'rescued', 'reservation', 'respond', 'rest', 'restaurant', 'revenge', 'rhode', 'right', 'rip', 'roberto', 'robotic', 'roger', 'romantic', 'room', \"room's\", 'rough', 'routines', 'rub', 'run', 'runyon', 'rush', 'sad', 'safe', 'said', 'samantha', 'same', 'saturday', 'save', 'saved', 'saving', 'saw', 'say', 'saying', 'says', 'scaring', 'scary', 'school', 'science', 'scoffs', 'scratched', 'second', 'see', 'seeing', 'seem', 'seems', 'seen', 'sees', 'selections', 'selfish', 'send', 'sending', 'sense', 'sensitive', 'sent', 'serious', 'service', 'set', 'seven', 'several', 'sex', 'sexual', 'sexy', 'share', 'shared', 'sharing', 'she', \"she'd\", \"she's\", 'shiny', 'ship', 'shipping', 'shirt', 'shit', 'shoes', 'shopping', 'should', \"shouldn't\", 'show', 'shown', 'shut', 'sick', 'sighs', 'sign', 'signed', 'signer', 'signing', 'silence', 'silly', 'simply', 'simultaneously', 'since', 'singing', 'sit', 'sitting', 'six', 'sixteen', 'size', 'skill', 'skin', 'skull', 'sleep', 'slice', 'slow', 'slower', 'slowly', 'smart', 'smoothie', 'sneeze', 'sneezes', 'snickers', 'so', 'socially', 'sofa', 'soft', 'softly', 'software', 'somber', 'some', 'somebody', \"somebody's\", 'somehow', 'someone', 'someplace', 'something', \"something's\", 'sometime', 'sometimes', 'somewhere', 'son', 'song', 'soon', 'sorry', 'sort', 'sound', 'sounded', 'sounds', 'space', 'spaces', 'speak', 'speakers', 'speaking', 'spelling', 'spend', 'spin', 'spinning', 'split', 'spoon', 'stab', 'stain', 'stalk', 'starry', 'start', 'started', 'starting', 'stating', 'statistically', 'still', 'stoked', 'stomp', 'stop', 'story', 'strange', 'strong', 'struggling', 'stubborn', 'stuck', 'stuff', 'stupid', 'subs', 'subscene', 'subtitle', 'successful', 'such', 'sugar', 'suggested', 'sunday', 'suppose', 'supposed', 'sure', 'surprise', 'surrogate', 'swallowed', 'sweet', 'sweetest', 'sweetheart', 'sweetie', 'synaptic', 'system', 'take', 'taken', 'takes', 'taking', 'talk', 'talked', 'talking', 'tank', 'taste', 'tastes', 'tatiana', 'tea', 'tears', 'teeth', 'tell', 'telling', 'tempo', 'terrible', 'test', 'tethered', 'than', 'thank', 'thanks', 'that', \"that's\", 'the', 'their', 'them', 'then', 'theo', 'theodore', 'there', \"there's\", 'these', 'thesis', 'they', \"they're\", \"they've\", 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'this', 'those', 'though', 'thought', 'thoughts', 'thousand', 'three', 'through', 'tiger', 'tightly', 'time', 'times', 'tiny', 'tips', 'tired', 'to', 'today', 'together', 'toilets', 'told', 'tomorrow', 'tone', 'tongue', 'tonight', 'too', 'took', 'tooth', 'total', 'totally', 'touch', 'touching', 'traveling', 'treating', 'trees', 'trick', 'tried', 'trip', 'trouble', 'true', 'truly', 'trust', 'try', 'trying', 'tunnel', 'turn', 'turning', 'turns', 'tv', 'twice', 'two', 'twombly', 'ugh', 'uh', 'ukulele', 'um', 'un', 'uncomfortable', 'under', 'understand', 'understandable', 'undo', 'unfair', 'unformed', 'unless', 'unsettling', 'up', 'upbeat', 'upgrade', 'upon', 'upset', 'urgent', 'us', 'use', 'used', 'usually', 'vacation', 'vegetables', 'verbally', 'version', 'versions', 'very', 'video', 'voice', 'volatile', 'vow', 'wadsworth', 'wait', 'waiting', 'wake', 'waking', 'walk', 'walked', 'walking', 'wallow', 'wanna', 'want', 'wanted', 'wants', 'was', \"wasn't\", 'waste', 'watch', 'watching', 'watts', 'way', 'ways', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'wednesday', 'week', 'weekend', 'weekly', 'weeks', 'weighed', 'weight', 'weird', 'well', 'went', 'were', \"weren't\", 'wet', 'what', \"what's\", 'whatever', 'when', 'where', \"where'd\", 'wherever', 'whew', 'while', 'whisper', 'whispering', 'whistling', 'white', 'who', \"who's\", 'whole', 'whoo', 'why', 'wife', 'will', 'willie', 'wilsons', 'wish', 'with', 'without', 'woke', 'woman', 'women', \"won't\", 'wonderful', 'word', 'words', 'work', 'worked', 'working', 'world', \"world's\", 'worried', 'worrier', 'worry', 'worst', 'worth', 'would', \"would've\", \"wouldn't\", 'wow', 'write', 'writer', 'writing', 'written', 'wrong', 'wrote', 'yeah', 'year', 'years', 'yep', 'yes', 'yet', 'yikes', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'younger', 'your', 'yours', 'yourself', '<PAD>']\n",
      "[573, 80, 1382, 332, 18, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# File path\n",
    "file_path = r\"./Data/SRT/timestamps.txt\"\n",
    "\n",
    "\n",
    "# Regular expression\n",
    "pattern = r'(\\d{3}|\\d{4})\\s+(\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3})\\s+([\\s\\S]*?)(?=</font>|</i></font>)'\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Find all matches\n",
    "matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "\n",
    "data_dict = {match[0]: {\"Timestamp\": match[1], \"Text\": match[2]} for match in matches}\n",
    "\n",
    "vocab = []\n",
    "\n",
    "for key, value in data_dict.items():\n",
    "    value['Text'] = value['Text'].replace('\\n', ' ').replace('/', '').replace('<b><font color=\"#ffff00\">', '').replace('<b><font color=\"#ffff00\"><i>', '').replace('</i></font></b>', '').replace('<i>','').lower()\n",
    "    value['Text'] = re.findall(r'\\b\\w+\\'?\\w*|[.,!?]', value['Text'])\n",
    "    vocab += value['Text']\n",
    "\n",
    "print(len(data_dict['359']['Text']))\n",
    "# Assuming data_dict is your dictionary\n",
    "max_text_key = max(data_dict, key=lambda k: len(data_dict[k]['Text']))\n",
    "max_text = data_dict[max_text_key]['Text']\n",
    "\n",
    "print(f\"Key with max 'Text' length: {max_text_key}\")\n",
    "print(f\"Text: {len(max_text)} {max_text}\")\n",
    "\n",
    "#get each unique value of the dictionary \n",
    "vocab_sorted = sorted(set(vocab))\n",
    "vocab_sorted.append('<PAD>')\n",
    "print(vocab_sorted)\n",
    "\n",
    "\n",
    "#take the vocabulary and match it with an integer\n",
    "tokenizer = {word: i for i, word in enumerate(vocab_sorted)}\n",
    "int_to_str = {i: word for i, word in enumerate(vocab_sorted)}\n",
    "\n",
    "\n",
    "#make encoder and decoder\n",
    "def encode(sentence, tokenizer, max_text_length, pad_token=\"<PAD>\"):\n",
    "    # Encode the sentence\n",
    "    encoded_sentence = [tokenizer[word] for word in sentence]\n",
    "    \n",
    "    # If the sentence is shorter than max_text_length, pad it\n",
    "    if len(encoded_sentence) < max_text_length:\n",
    "        padding_needed = max_text_length - len(encoded_sentence)\n",
    "        encoded_sentence += [tokenizer[pad_token]] * padding_needed\n",
    "    \n",
    "    return encoded_sentence\n",
    "\n",
    "\n",
    "decode = lambda l: [int_to_str[i] for i in l]\n",
    "\n",
    "print((encode(data_dict['171']['Text'], tokenizer, len(max_text))))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to tokenizer.txt\n"
     ]
    }
   ],
   "source": [
    "# Assuming tokenizer is defined as follows:\n",
    "tokenizer = {i: word for i, word in enumerate(vocab_sorted)}\n",
    "\n",
    "# File path to save the tokenizer\n",
    "file_path = \"tokenizer.txt\"\n",
    "\n",
    "# Write the tokenizer to a text file\n",
    "with open(file_path, \"w\") as file:\n",
    "    for key, value in tokenizer.items():\n",
    "        file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(f\"Tokenizer saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
