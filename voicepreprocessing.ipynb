{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydub librosa numpy torchaudio speechbrain\n",
    "pip install webrtcvad\n",
    "\n",
    "import webrtcvad\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "input_file = r'./Audio Files/Her/Her.mp4'\n",
    "output_file = r'./Audio Files/Her/HerOutput.wav'\n",
    "\n",
    "def convert_to_wav(input_file, output_file):\n",
    "    audio = AudioSegment.from_file(\n",
    "        input_file,\n",
    "        format = 'mp4',\n",
    "        )\n",
    "    audio.export(output_file, format='wav')\n",
    "\n",
    "\n",
    "convert_to_wav(input_file, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time in ms is 1511720 and end time in ms is 1512807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='./Data/Training_Samples/sample1.wav'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Analyzing Speech (e.g., Phoneme Conversion):\n",
    "\n",
    "    Lower hop_length:\n",
    "        Provides better time resolution, allowing the model to capture finer details, especially crucial for accurately segmenting phonemes and detecting quick transitions between them.\n",
    "        More frames = higher granularity in time.\n",
    "    Higher n_fft:\n",
    "        Provides better frequency resolution, allowing the model to capture finer frequency details, which are important for distinguishing between different phonemes based on their spectral features.\n",
    "        More frequency bins = more precise frequency representation.\n",
    "\n",
    "For Generating Speech (e.g., Speech Synthesis):\n",
    "\n",
    "    Higher hop_length:\n",
    "        Reduces the number of frames, which decreases computational cost. It also provides less granular time resolution, but this is often acceptable for generating smoother speech and avoiding overfitting on small time variations.\n",
    "    Lower n_fft:\n",
    "        Provides coarser frequency resolution, which can help model more general frequency patterns, reducing computational complexity while still maintaining enough frequency detail for natural speech synthesis.\n",
    "\n",
    "Summary:\n",
    "\n",
    "    For analysis: Lower hop_length and higher n_fft (better time and frequency resolution).\n",
    "    For generation: Higher hop_length and lower n_fft (better computational efficiency, but with slightly reduced resolution in both time and frequency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting WAV --> signal (2D Numpy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (23969,) [ 0.00146869  0.00227872  0.00214889 ... -0.00629799 -0.00731505\n",
      "  0.        ]\n",
      "torch.Size([128, 94])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting tensors back into WAV files\n",
    "\n",
    "torch.Size([80, 47]) \n",
    "n_fft = 1024  # Window length\n",
    "hop_length = 512 # Hop length\n",
    "n_mels = 80\n",
    "\n",
    "\n",
    "torch.Size([128, 94])\n",
    "n_fft = 2048  # Window length\n",
    "hop_length = 256 # Hop length\n",
    "n_mels = 128\n",
    "\n",
    "Number of Frames=⌊(Signal Length−n_fft)​/hop_length⌋+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.waveshow(y=signal, sr=sr)\n",
    "plt.title('Signal')\n",
    "plt.xlabel('Time (samples)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "img = librosa.display.specshow(\n",
    "    log_mel_spectrogram, \n",
    "    sr=sr, \n",
    "    hop_length=hop_length, \n",
    "    x_axis='time', \n",
    "    y_axis='mel', \n",
    "    fmax=8000, \n",
    "    cmap='magma'\n",
    ")\n",
    "plt.colorbar(img, label='DB')  # Add a label to the colorbar\n",
    "plt.title('Log Mel Spectrogram')\n",
    "plt.xlabel('Time (s)')  # Label for the x-axis\n",
    "plt.ylabel('Frequency (Hz)')  # Label for the y-axis\n",
    "plt.grid(True, linestyle='--', alpha=0.6)  # Optional: add grid for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Reverse Process\n",
    "mel_spectrogram_reversed = librosa.db_to_power(log_mel_spectrogram, ref=np.max(mel_spectrogram))\n",
    "linear_spectrogram = librosa.feature.inverse.mel_to_stft(mel_spectrogram_reversed, sr=sr, n_fft=n_fft)\n",
    "\n",
    "# Use Griffin-Lim to reconstruct the signal\n",
    "reconstructed_signal = librosa.griffinlim(linear_spectrogram, hop_length=hop_length, n_iter=64)\n",
    "\n",
    "# Save reconstructed audio\n",
    "sf.write('reconstructed_audio.wav', reconstructed_signal, sr)\n",
    "\n",
    "\n",
    "\n",
    "output_file = 'output.wav'  # Specify the output file name\n",
    "\n",
    "# Ensure the signal is scaled to the appropriate range for WAV files\n",
    "# WAV files typically expect 16-bit PCM, so we scale the signal to int16\n",
    "scaled_signal = (signal * 32767).astype(np.int16)\n",
    "\n",
    "# Write the WAV file\n",
    "write(output_file, sr, scaled_signal)\n",
    "\n",
    "print(f\"WAV file written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[753080, 755287]\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "import re\n",
    "import utils\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf \n",
    "\n",
    "def read_srt_file():\n",
    "    file_path = utils.get_srt()\n",
    "    if not file_path:\n",
    "        raise FileNotFoundError(\"SRT file path not found in the configuration.\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "input_file = read_srt_file()\n",
    "\n",
    "# Regular expression\n",
    "pattern = r'(\\d{3}|\\d{4})\\s+(\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3})\\s+([\\s\\S]*?)(?=</font>|</i></font>)'\n",
    "\n",
    "# Find all matches\n",
    "matches = re.findall(pattern, input_file, re.DOTALL)\n",
    "\n",
    "data_dict = {match[0]: {\"Timestamp\": match[1], \"Text\": match[2]} for match in matches}\n",
    "\n",
    "vocab = []\n",
    "\n",
    "#cleaning transcription by moving values from SRT file\n",
    "for key, value in data_dict.items():\n",
    "    value['Text'] = (\n",
    "        value['Text']\n",
    "        .replace('\\n', ' ')\n",
    "        .replace('/', '')\n",
    "        .replace('<b><font color=\"#ffff00\">', '')\n",
    "        .replace('<b><font color=\"#ffff00\"><i>', '')\n",
    "        .replace('</i></font></b>', '')\n",
    "        .replace('<i>','')\n",
    "        .lower()\n",
    "    )\n",
    "    value['Text'] = [\n",
    "        '<SPACE>' if token.isspace() else token # convert spaces into <SPACE>\n",
    "        for token in re.findall(r'\\b\\w+\\'?\\w*|[.,!?]|\\s', value['Text']) #convert text into list with words, <SPACE>, and .,!?\n",
    "    ]\n",
    "    vocab += value['Text']\n",
    "    value['Timestamp'] = [utils.t2ms(split) for split in value['Timestamp'].split('-->')]\n",
    "\n",
    "print(data_dict['167']['Timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m cut_audio \u001b[38;5;241m=\u001b[39m audio[start_t:end_t]\n\u001b[1;32m      8\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Data/Training_Samples/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m sf\u001b[38;5;241m.\u001b[39mwrite(output_file, cut_audio, \u001b[38;5;241m22050\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:1197\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_line:\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_suspend(thread, step_cmd, original_step_cmd\u001b[38;5;241m=\u001b[39minfo\u001b[38;5;241m.\u001b[39mpydev_original_step_cmd)\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_wait_suspend(thread, frame, event, arg)\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_return:  \u001b[38;5;66;03m# return event\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m     back \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_back\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdo_wait_suspend(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "whole_audio = utils.get_whole_audio()\n",
    "audio = librosa.load(whole_audio)\n",
    "\n",
    "for key, value in data_dict.items():\n",
    "    # Extract the start and end times from the \"Timestamp\" key\n",
    "    start_t, end_t = value[\"Timestamp\"]\n",
    "    cut_audio = audio[start_t:end_t]\n",
    "    output_file = f\"./Data/Training_Samples/{key}.wav\"\n",
    "    sf.write(output_file, cut_audio, 22050)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123', '']\n"
     ]
    }
   ],
   "source": [
    "file = '123.wav'\n",
    "trunc_file = file.split('.wav')\n",
    "print(trunc_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
